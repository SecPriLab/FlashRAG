"""
å®Œæ•´çš„ MIA Pipeline æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰åŠŸèƒ½ï¼š
1. æ•°æ®å‡†å¤‡
2. ç´¢å¼•æ„å»º
3. æ£€ç´¢åŠŸèƒ½ï¼ˆè¿”å›æ–‡æ¡£IDï¼‰
4. ç”ŸæˆåŠŸèƒ½ï¼ˆè¿”å›logitsï¼‰
5. å¤šè½®å¯¹è¯
"""

import os
import json
import torch
from flashrag.config import Config
from flashrag.utils import get_generator, get_retriever
from flashrag.prompt import PromptTemplate


def test_data_preparation():
    """æµ‹è¯•æ•°æ®å‡†å¤‡åŠŸèƒ½"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 1: Data Preparation")
    print("="*70)

    corpus_path = '/home/user/FlashRAG/datasets/scifact/corpus.jsonl'

    if not os.path.exists(corpus_path):
        print("âŒ corpus.jsonl not found. Please place it in datasets/scifact/")
        return False

    # è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬
    print("Running prepare_mia_data.py...")
    os.system('cd /home/user/FlashRAG && python prepare_mia_data.py')

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    expected_files = [
        'datasets/scifact/member_samples.jsonl',
        'datasets/scifact/nonmember_samples.jsonl',
        'datasets/scifact/index_corpus.jsonl',
        'datasets/scifact/queries.jsonl'
    ]

    all_exist = True
    for file in expected_files:
        full_path = f'/home/user/FlashRAG/{file}'
        if os.path.exists(full_path):
            print(f"âœ… {file} created")
        else:
            print(f"âŒ {file} NOT found")
            all_exist = False

    return all_exist


def test_index_building():
    """æµ‹è¯•ç´¢å¼•æ„å»º"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 2: Index Building")
    print("="*70)

    # è¿è¡Œç´¢å¼•æ„å»ºè„šæœ¬
    print("Running build_index.py...")
    os.system('cd /home/user/FlashRAG && python build_index.py')

    # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
    index_path = '/home/user/FlashRAG/indexes/scifact/index'
    if os.path.exists(index_path):
        print(f"âœ… Index file created at {index_path}")
        return True
    else:
        print(f"âŒ Index file NOT found at {index_path}")
        return False


def test_retriever_with_doc_ids():
    """æµ‹è¯•æ£€ç´¢å™¨è¿”å›æ–‡æ¡£ID"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 3: Retriever with Document IDs")
    print("="*70)

    config_dict = {
        'retrieval_method': 'bge',
        'retrieval_model_path': 'BAAI/bge-large-en-v1.5',
        'corpus_path': '/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl',
        'index_path': '/home/user/FlashRAG/indexes/scifact',
        'retrieval_topk': 3,
        'retrieval_batch_size': 256,
        'retrieval_pooling_method': 'mean',
        'retrieval_use_fp16': True,
        'faiss_gpu': True,
        'use_sentence_transformer': False,
    }

    try:
        config = Config(config_dict=config_dict)
        retriever = get_retriever(config)

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "What is myelodysplasia?"
        print(f"\nğŸ“ Test query: {test_query}")

        # æµ‹è¯•è¿”å›æ–‡æ¡£ID
        results, scores, doc_ids = retriever._batch_search(
            query=[test_query],
            num=3,
            return_score=True,
            return_doc_ids=True
        )

        print(f"\nğŸ“Š Results:")
        print(f"  - Retrieved {len(results[0])} documents")
        print(f"  - Document IDs: {doc_ids[0]}")
        print(f"  - Scores: {[f'{s:.4f}' for s in scores[0]]}")

        for idx, (doc, doc_id, score) in enumerate(zip(results[0], doc_ids[0], scores[0]), 1):
            print(f"\n  Document {idx} (ID: {doc_id}, Score: {score:.4f}):")
            title = doc.get('title', 'N/A')
            print(f"    Title: {title[:100]}...")

        return True

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_generator_with_logits():
    """æµ‹è¯•ç”Ÿæˆå™¨è¿”å›logits"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 4: Generator with Logits")
    print("="*70)

    config_dict = {
        'generator_model': 'llama3.1-8b-instruct',
        'generator_model_path': '/remote-home/RAG_Privacy/model/meta-llama/Llama-3.1-8B-Instruct',
        'generator_max_input_len': 2048,
        'generator_batch_size': 1,
        'generation_params': {
            'do_sample': False,
        },
        'framework': 'hf',
        'device': 'cuda',
    }

    try:
        config = Config(config_dict=config_dict)
        generator = get_generator(config)

        # æµ‹è¯•è¾“å…¥
        test_input = "Answer the following question with only one letter (A, B, C, D, or E): What is the capital of France?"

        print(f"\nğŸ“ Test input: {test_input}")

        # ç”Ÿæˆå¹¶è¿”å›logits
        output = generator.generate(
            [test_input],
            return_dict=True,
            max_new_tokens=10
        )

        response = output['responses'][0]
        generated_logits = output['generated_token_logits'][0]  # [num_tokens, vocab_size]

        print(f"\nğŸ“Š Results:")
        print(f"  - Response: {response}")
        print(f"  - Logits shape: {generated_logits.shape}")
        print(f"  - First token logits shape: {generated_logits[0].shape}")

        # è·å– A-E çš„ token IDs å’Œæ¦‚ç‡
        answer_tokens = ['A', 'B', 'C', 'D', 'E']
        answer_token_ids = {
            token: generator.tokenizer.convert_tokens_to_ids(token)
            for token in answer_tokens
        }

        print(f"\n  Answer token IDs: {answer_token_ids}")

        # æå–ç¬¬ä¸€ä¸ªtokençš„æ¦‚ç‡
        first_token_logits = generated_logits[0]
        answer_probs = {
            token: first_token_logits[token_id].item()
            for token, token_id in answer_token_ids.items()
        }

        print(f"  Answer probabilities:")
        for token, prob in answer_probs.items():
            print(f"    {token}: {prob:.6f}")

        return True

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multi_turn_chat():
    """æµ‹è¯•å¤šè½®å¯¹è¯åŠŸèƒ½"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 5: Multi-Turn Chat")
    print("="*70)

    try:
        from mia_multi_turn_chat import create_mia_chat

        # åˆ›å»ºå¯¹è¯å®ä¾‹
        chat = create_mia_chat()

        # æµ‹è¯•å¯¹è¯ 1: å¸¦æ£€ç´¢
        print("\nğŸ“Œ Turn 1: WITH retrieval")
        print("-" * 70)
        result1 = chat.chat(
            user_query="What is the role of MDSC in myelodysplasia?",
            use_retrieval=True,
            topk=3
        )

        # æµ‹è¯•å¯¹è¯ 2: ä¸å¸¦æ£€ç´¢
        print("\nğŸ“Œ Turn 2: WITHOUT retrieval")
        print("-" * 70)
        result2 = chat.chat(
            user_query="Can you elaborate on that?",
            use_retrieval=False
        )

        # æ‰“å°å¯¹è¯å†å²
        chat.print_conversation_history()

        # éªŒè¯ç»“æœ
        print("\nğŸ“Š Verification:")
        print(f"  - Turn 1 retrieved docs: {result1['retrieved_doc_ids']}")
        print(f"  - Turn 1 answer probs: {result1['answer_probs']}")
        print(f"  - Turn 2 response: {result2['response']}")

        return True

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_single_query_pipeline():
    """æµ‹è¯•å•ä¸ªæŸ¥è¯¢çš„å®Œæ•´pipeline"""
    print("\n" + "="*70)
    print("ğŸ§ª Test 6: Complete Single Query Pipeline")
    print("="*70)

    config_dict = {
        # Generator é…ç½®
        'generator_model': 'llama3.1-8b-instruct',
        'generator_model_path': '/remote-home/RAG_Privacy/model/meta-llama/Llama-3.1-8B-Instruct',
        'generator_max_input_len': 2048,
        'generator_batch_size': 1,
        'generation_params': {
            'do_sample': False,
        },

        # Retriever é…ç½®
        'retrieval_method': 'bge',
        'retrieval_model_path': 'BAAI/bge-large-en-v1.5',
        'corpus_path': '/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl',
        'index_path': '/home/user/FlashRAG/indexes/scifact',
        'retrieval_topk': 3,
        'retrieval_batch_size': 256,
        'retrieval_pooling_method': 'mean',
        'retrieval_use_fp16': True,
        'faiss_gpu': True,
        'use_sentence_transformer': False,

        # å…¶ä»–é…ç½®
        'framework': 'hf',
        'device': 'cuda',
    }

    try:
        config = Config(config_dict=config_dict)
        retriever = get_retriever(config)
        generator = get_generator(config)
        prompt_template = PromptTemplate(
            config,
            system_prompt=PromptTemplate.mia_system_prompt,
            user_prompt=PromptTemplate.base_user_prompt
        )

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "What is the function of BC1 RNA in ID element amplification?"

        print(f"\nğŸ“ Query: {test_query}")

        # 1. æ£€ç´¢
        print("\nğŸ” Step 1: Retrieval")
        results, scores, doc_ids = retriever._batch_search(
            query=[test_query],
            num=3,
            return_score=True,
            return_doc_ids=True
        )

        print(f"  Retrieved document IDs: {doc_ids[0]}")

        # 2. æ„å»º prompt
        print("\nğŸ“„ Step 2: Prompt Construction")
        input_prompt = prompt_template.get_string(
            question=test_query,
            retrieval_result=results[0]
        )
        print(f"  Prompt length: {len(input_prompt)} characters")

        # 3. ç”Ÿæˆ
        print("\nğŸ¤– Step 3: Generation")
        output = generator.generate(
            [input_prompt],
            return_dict=True,
            max_new_tokens=10
        )

        response = output['responses'][0]
        logits = output['generated_token_logits'][0]

        print(f"  Response: {response}")
        print(f"  Logits shape: {logits.shape}")

        # 4. åˆ†æç­”æ¡ˆ
        print("\nğŸ“Š Step 4: Answer Analysis")
        answer_tokens = ['A', 'B', 'C', 'D', 'E']
        answer_token_ids = {
            token: generator.tokenizer.convert_tokens_to_ids(token)
            for token in answer_tokens
        }

        first_token_logits = logits[0]
        answer_probs = {
            token: first_token_logits[token_id].item()
            for token, token_id in answer_token_ids.items()
        }

        print("  Answer probabilities:")
        for token, prob in sorted(answer_probs.items(), key=lambda x: x[1], reverse=True):
            print(f"    {token}: {prob:.6f}")

        print("\nâœ… Complete pipeline test passed!")
        return True

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("ğŸš€ MIA Pipeline - Complete Testing Suite")
    print("="*70)

    tests = [
        # ("Data Preparation", test_data_preparation),
        # ("Index Building", test_index_building),
        ("Retriever with Doc IDs", test_retriever_with_doc_ids),
        ("Generator with Logits", test_generator_with_logits),
        ("Multi-Turn Chat", test_multi_turn_chat),
        ("Complete Single Query Pipeline", test_single_query_pipeline),
    ]

    results = {}

    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = "âœ… PASSED" if result else "âŒ FAILED"
        except Exception as e:
            results[test_name] = f"âŒ ERROR: {str(e)}"

    # æ‰“å°æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“‹ Test Summary")
    print("="*70)
    for test_name, result in results.items():
        print(f"  {test_name}: {result}")
    print("="*70 + "\n")


if __name__ == '__main__':
    main()
