"""
å¤šè½®å¯¹è¯æ¥å£ - ç”¨äºæˆå‘˜æ¨ç†æ”»å‡»å®éªŒ
æ”¯æŒï¼š
1. é€‰æ‹©æ˜¯å¦æ£€ç´¢
2. è¿”å›æ–‡æ¡£ ID
3. è¿”å› logits
4. ç»´æŠ¤å¯¹è¯å†å²
"""

import torch
from typing import List, Dict, Optional, Tuple
from flashrag.config import Config
from flashrag.utils import get_generator, get_retriever
from flashrag.prompt import PromptTemplate
from flashrag.retriever.utils import load_corpus


class MIAMultiTurnChat:
    """å¤šè½®å¯¹è¯ç±»ï¼Œæ”¯æŒæˆå‘˜æ¨ç†æ”»å‡»å®éªŒ"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–å¤šè½®å¯¹è¯ç³»ç»Ÿ

        Args:
            config: FlashRAG é…ç½®å¯¹è±¡
        """
        self.config = config
        self.generator = get_generator(config)
        self.retriever = get_retriever(config)

        # ä½¿ç”¨ MIA ä¸“ç”¨çš„æç¤ºè¯
        self.prompt_template = PromptTemplate(
            config,
            system_prompt=PromptTemplate.mia_system_prompt,
            user_prompt=PromptTemplate.base_user_prompt
        )

        # ä¸å¸¦æ£€ç´¢çš„æç¤ºè¯æ¨¡æ¿
        self.no_retrieval_prompt_template = PromptTemplate(
            config,
            system_prompt="You are a helpful assistant. Answer the question based on your knowledge. "
                         "Your answer should be short and concise. You can only output one letter from A, B, C, D, or E as your answer.",
            user_prompt="Question: {question}"
        )

        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.messages = []

        # è·å–ç­”æ¡ˆ token IDs (A, B, C, D, E)
        self.answer_tokens = ['A', 'B', 'C', 'D', 'E']
        self.answer_token_ids = {
            token: self.generator.tokenizer.convert_tokens_to_ids(token)
            for token in self.answer_tokens
        }
        # åå‘æ˜ å°„
        self.inv_answer_token_ids = {v: k for k, v in self.answer_token_ids.items()}

        print("âœ… MIA Multi-Turn Chat initialized successfully!")

    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.messages = []
        print("ğŸ”„ Conversation history reset")

    def format_documents(self, documents: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„æ–‡æ¡£å­—ç¬¦ä¸²
        """
        formatted_docs = []
        for idx, doc in enumerate(documents, 1):
            # å¤„ç†ä¸åŒçš„æ–‡æ¡£æ ¼å¼
            if 'text' in doc:
                content = doc['text']
            elif 'contents' in doc:
                content = doc['contents']
            else:
                content = str(doc)

            formatted_docs.append(f"Document {idx}:\n{content}\n")

        return "\n".join(formatted_docs)

    def chat(
        self,
        user_query: str,
        use_retrieval: bool = True,
        topk: int = 3,
        return_details: bool = True
    ) -> Dict:
        """
        æ‰§è¡Œä¸€è½®å¯¹è¯

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            use_retrieval: æ˜¯å¦ä½¿ç”¨æ£€ç´¢
            topk: æ£€ç´¢æ–‡æ¡£æ•°é‡
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆæ–‡æ¡£IDã€logitsç­‰ï¼‰

        Returns:
            åŒ…å«å›ç­”å’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        result = {
            'user_query': user_query,
            'use_retrieval': use_retrieval,
            'response': '',
            'retrieved_doc_ids': None,
            'retrieved_documents': None,
            'logits': None,
            'answer_probs': None,
            'predicted_answer': None
        }

        # 1. æ£€ç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if use_retrieval:
            print(f"ğŸ” Retrieving top-{topk} documents...")

            # ä½¿ç”¨ä¿®æ”¹åçš„æ£€ç´¢å™¨ï¼Œè¿”å›æ–‡æ¡£ID
            retrieved_docs, scores, doc_ids = self.retriever._batch_search(
                query=[user_query],
                num=topk,
                return_score=True,
                return_doc_ids=True
            )

            retrieved_docs = retrieved_docs[0]  # è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœ
            doc_ids = doc_ids[0]
            scores = scores[0]

            result['retrieved_doc_ids'] = doc_ids
            result['retrieved_documents'] = retrieved_docs

            print(f"ğŸ“„ Retrieved document IDs: {doc_ids}")

            # æ ¼å¼åŒ–æ–‡æ¡£
            formatted_docs = self.format_documents(retrieved_docs)

            # æ„å»ºåŒ…å«æ£€ç´¢ç»“æœçš„ prompt
            input_prompt = self.prompt_template.get_string(
                question=user_query,
                retrieval_result=retrieved_docs
            )

            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            self.messages.append({
                "role": "user",
                "content": user_query,
                "retrieved_docs": doc_ids  # ä¿å­˜æ£€ç´¢åˆ°çš„æ–‡æ¡£ID
            })

        else:
            print("ğŸ’¬ Direct generation without retrieval...")

            # ä¸ä½¿ç”¨æ£€ç´¢ï¼Œç›´æ¥æ·»åŠ ç”¨æˆ·æŸ¥è¯¢
            self.messages.append({
                "role": "user",
                "content": user_query
            })

            # ä½¿ç”¨ä¸å¸¦æ£€ç´¢çš„æç¤ºè¯
            input_prompt = self.no_retrieval_prompt_template.get_string(
                question=user_query
            )

        # 2. ç”Ÿæˆå›ç­”
        print("ğŸ¤– Generating response...")

        # ä½¿ç”¨ return_dict=True è·å– logits
        generation_output = self.generator.generate(
            [input_prompt],
            return_dict=True,
            max_new_tokens=10  # å› ä¸ºåªéœ€è¦è¾“å‡ºä¸€ä¸ªå­—æ¯
        )

        response = generation_output['responses'][0]
        generated_token_logits = generation_output['generated_token_logits'][0]  # [num_tokens, vocab_size]

        result['response'] = response

        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        self.messages.append({
            "role": "assistant",
            "content": response
        })

        # 3. æå–ç­”æ¡ˆ logits å’Œæ¦‚ç‡
        if return_details:
            # è·å–ç¬¬ä¸€ä¸ªç”Ÿæˆçš„ token çš„ logitsï¼ˆå³ç­”æ¡ˆï¼‰
            first_token_logits = generated_token_logits[0]  # [vocab_size]

            # æå– A-E çš„æ¦‚ç‡
            answer_probs = {}
            for token, token_id in self.answer_token_ids.items():
                answer_probs[token] = first_token_logits[token_id].item()

            result['logits'] = first_token_logits.cpu()
            result['answer_probs'] = answer_probs

            # é¢„æµ‹çš„ç­”æ¡ˆ
            first_token_id = self.generator.tokenizer.encode(
                response,
                add_special_tokens=False
            )[0] if response else None

            predicted_answer = self.inv_answer_token_ids.get(first_token_id, response[0] if response else "N/A")
            result['predicted_answer'] = predicted_answer

            print(f"ğŸ“Š Answer probabilities: {answer_probs}")
            print(f"âœ¨ Predicted answer: {predicted_answer}")

        print(f"ğŸ’¡ Response: {response}\n")

        return result

    def get_conversation_history(self) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.messages

    def print_conversation_history(self):
        """æ‰“å°å¯¹è¯å†å²"""
        print("\n" + "="*50)
        print("ğŸ“œ Conversation History")
        print("="*50)
        for idx, msg in enumerate(self.messages, 1):
            role = msg['role'].upper()
            content = msg['content']
            print(f"\n[{idx}] {role}:")
            print(f"  {content}")
            if 'retrieved_docs' in msg:
                print(f"  ğŸ“ Retrieved Docs: {msg['retrieved_docs']}")
        print("="*50 + "\n")


def create_mia_chat(
    model_path: str = "/remote-home/RAG_Privacy/model/meta-llama/Llama-3.1-8B-Instruct",
    retrieval_method: str = "bge",
    corpus_path: str = "/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl",
    index_path: str = "/home/user/FlashRAG/indexes/scifact",
    retrieval_topk: int = 3
) -> MIAMultiTurnChat:
    """
    åˆ›å»º MIA å¤šè½®å¯¹è¯å®ä¾‹çš„ä¾¿æ·å‡½æ•°

    Args:
        model_path: LLM æ¨¡å‹è·¯å¾„
        retrieval_method: æ£€ç´¢æ–¹æ³•
        corpus_path: è¯­æ–™åº“è·¯å¾„
        index_path: ç´¢å¼•è·¯å¾„
        retrieval_topk: æ£€ç´¢æ–‡æ¡£æ•°é‡

    Returns:
        MIAMultiTurnChat å®ä¾‹
    """
    config_dict = {
        # Generator é…ç½®
        'generator_model': 'llama3.1-8b-instruct',
        'generator_model_path': model_path,
        'generator_max_input_len': 2048,
        'generator_batch_size': 1,
        'generation_params': {
            'temperature': 0.7,
            'top_p': 0.9,
            'do_sample': False,  # ç¡®å®šæ€§ç”Ÿæˆ
        },

        # Retriever é…ç½®
        'retrieval_method': retrieval_method,
        'retrieval_model_path': 'BAAI/bge-large-en-v1.5',
        'corpus_path': corpus_path,
        'index_path': index_path,
        'retrieval_topk': retrieval_topk,
        'retrieval_batch_size': 256,
        'retrieval_pooling_method': 'mean',
        'retrieval_use_fp16': True,
        'faiss_gpu': True,
        'use_sentence_transformer': False,

        # å…¶ä»–é…ç½®
        'framework': 'hf',
        'device': 'cuda',
        'gpu_id': 0,
    }

    config = Config(config_dict=config_dict)
    return MIAMultiTurnChat(config)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == '__main__':
    # åˆ›å»ºå¯¹è¯å®ä¾‹
    chat = create_mia_chat()

    print("\n" + "="*70)
    print("ğŸ¯ MIA Multi-Turn Chat - Membership Inference Attack Experiments")
    print("="*70 + "\n")

    # ç¤ºä¾‹ 1: å¸¦æ£€ç´¢çš„å¯¹è¯
    print("ğŸ“Œ Example 1: Chat WITH retrieval")
    print("-" * 70)
    result1 = chat.chat(
        user_query="What is the role of myeloid-derived suppressor cells in myelodysplasia?",
        use_retrieval=True,
        topk=3
    )

    # ç¤ºä¾‹ 2: ä¸å¸¦æ£€ç´¢çš„å¯¹è¯
    print("\nğŸ“Œ Example 2: Chat WITHOUT retrieval")
    print("-" * 70)
    result2 = chat.chat(
        user_query="Can you explain more about that?",
        use_retrieval=False
    )

    # ç¤ºä¾‹ 3: å†æ¬¡å¸¦æ£€ç´¢çš„å¯¹è¯
    print("\nğŸ“Œ Example 3: Another query WITH retrieval")
    print("-" * 70)
    result3 = chat.chat(
        user_query="How does diffusion tensor MRI assess cerebral white matter?",
        use_retrieval=True,
        topk=3
    )

    # æ‰“å°å®Œæ•´çš„å¯¹è¯å†å²
    chat.print_conversation_history()

    # é‡ç½®å¯¹è¯
    chat.reset_conversation()

    print("\nâœ… All examples completed!")
