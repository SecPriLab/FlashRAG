# FlashRAG æˆå‘˜æ¨ç†æ”»å‡» (MIA) å®éªŒæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ä¿®æ”¹åçš„ FlashRAG è¿›è¡Œæˆå‘˜æ¨ç†æ”»å‡»å®éªŒã€‚

## ğŸ“‹ ç›®å½•

1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [å®‰è£…è¦æ±‚](#å®‰è£…è¦æ±‚)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è¯¦ç»†ä½¿ç”¨è¯´æ˜](#è¯¦ç»†ä½¿ç”¨è¯´æ˜)
5. [API å‚è€ƒ](#api-å‚è€ƒ)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

æœ¬é¡¹ç›®å¯¹ FlashRAG è¿›è¡Œäº†ä»¥ä¸‹æ‰©å±•ï¼Œä»¥æ”¯æŒæˆå‘˜æ¨ç†æ”»å‡»å®éªŒï¼š

### 1. **æ•°æ®å‡†å¤‡**
- âœ… ä» corpus.jsonl éšæœºé‡‡æ ·æˆå‘˜å’Œéæˆå‘˜æ ·æœ¬
- âœ… åˆ›å»ºç”¨äºç´¢å¼•çš„è¯­æ–™åº“ï¼ˆæ’é™¤éæˆå‘˜æ ·æœ¬ï¼‰
- âœ… ç”ŸæˆæŸ¥è¯¢æ•°æ®é›†

### 2. **æ£€ç´¢å¢å¼º**
- âœ… ä½¿ç”¨ BGE ç¼–ç å™¨æ„å»º FAISS ç´¢å¼•
- âœ… **æ–°å¢**ï¼šæ£€ç´¢å™¨è¿”å›æ–‡æ¡£ ID
- âœ… æ”¯æŒ GPU åŠ é€Ÿ

### 3. **ç”Ÿæˆå¢å¼º**
- âœ… ä½¿ç”¨ Llama-3.1-8B-Instruct ç”Ÿæˆç­”æ¡ˆ
- âœ… **æ–°å¢**ï¼šè¿”å›ç”Ÿæˆçš„ logits
- âœ… æå–ç‰¹å®šç­”æ¡ˆï¼ˆA-Eï¼‰çš„æ¦‚ç‡

### 4. **å¤šè½®å¯¹è¯**
- âœ… æ”¯æŒé€‰æ‹©æ˜¯å¦æ£€ç´¢
- âœ… ç»´æŠ¤å¯¹è¯å†å²
- âœ… è¿”å›è¯¦ç»†çš„æ¨ç†ä¿¡æ¯

### 5. **æç¤ºè¯å®šåˆ¶**
- âœ… æ–°å¢ MIA ä¸“ç”¨æç¤ºè¯æ¨¡æ¿
- âœ… æ”¯æŒè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

---

## ğŸ“¦ å®‰è£…è¦æ±‚

### å‰ç½®æ¡ä»¶
```bash
# å·²å®‰è£…çš„åŒ…
- Python >= 3.8
- PyTorch >= 2.0
- CUDA (for GPU support)
- FlashRAG (pip install -e .)
- faiss-gpu
```

### éªŒè¯å®‰è£…
```python
from flashrag.config import Config
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline

config = Config()
print("FlashRAG installation successful!")
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1ï¼šå‡†å¤‡æ•°æ®

å°†æ‚¨çš„ `corpus.jsonl` æ–‡ä»¶æ”¾ç½®åœ¨ä»¥ä¸‹ä½ç½®ï¼š
```
FlashRAG/datasets/scifact/corpus.jsonl
```

corpus.jsonl æ ¼å¼ï¼š
```json
{"_id": "4983", "title": "...", "text": "...", "metadata": {}}
{"_id": "5836", "title": "...", "text": "...", "metadata": {}}
```

è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ï¼š
```bash
cd /home/user/FlashRAG
python prepare_mia_data.py
```

è¾“å‡ºæ–‡ä»¶ï¼š
- `datasets/scifact/member_samples.jsonl` - 1000ä¸ªæˆå‘˜æ ·æœ¬
- `datasets/scifact/nonmember_samples.jsonl` - 1000ä¸ªéæˆå‘˜æ ·æœ¬
- `datasets/scifact/index_corpus.jsonl` - ç´¢å¼•è¯­æ–™åº“
- `datasets/scifact/queries.jsonl` - æŸ¥è¯¢æ•°æ®é›†

### æ­¥éª¤ 2ï¼šæ„å»ºç´¢å¼•

```bash
python build_index.py
```

è¿™å°†ä½¿ç”¨ BGE ç¼–ç å™¨ä¸º `index_corpus.jsonl` æ„å»º FAISS ç´¢å¼•ã€‚

è¾“å‡ºï¼š
- `indexes/scifact/index` - FAISS ç´¢å¼•æ–‡ä»¶

### æ­¥éª¤ 3ï¼šæµ‹è¯•åŠŸèƒ½

```bash
python test_mia_pipeline.py
```

è¿™å°†è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ŒéªŒè¯ï¼š
- æ£€ç´¢å™¨è¿”å›æ–‡æ¡£ ID
- ç”Ÿæˆå™¨è¿”å› logits
- å¤šè½®å¯¹è¯åŠŸèƒ½
- å®Œæ•´çš„ pipeline

---

## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜

### 1. æ•°æ®å‡†å¤‡è¯¦è§£

`prepare_mia_data.py` è„šæœ¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```python
# ä¸»è¦åŠŸèƒ½
def sample_member_nonmember(corpus, member_size=1000, nonmember_size=1000, seed=42):
    """
    Args:
        corpus: å®Œæ•´çš„æ–‡æ¡£åˆ—è¡¨
        member_size: æˆå‘˜æ ·æœ¬æ•°é‡ (é»˜è®¤ 1000)
        nonmember_size: éæˆå‘˜æ ·æœ¬æ•°é‡ (é»˜è®¤ 1000)
        seed: éšæœºç§å­ (é»˜è®¤ 42)

    Returns:
        member_docs: æˆå‘˜æ ·æœ¬åˆ—è¡¨
        nonmember_docs: éæˆå‘˜æ ·æœ¬åˆ—è¡¨
        index_corpus: ç”¨äºå»ºç«‹ç´¢å¼•çš„è¯­æ–™åº“ï¼ˆæ’é™¤éæˆå‘˜ï¼‰
    """
```

**è‡ªå®šä¹‰é‡‡æ ·å¤§å°ï¼š**
```python
# ä¿®æ”¹ prepare_mia_data.py ä¸­çš„å‚æ•°
member_docs, nonmember_docs, index_corpus = sample_member_nonmember(
    corpus,
    member_size=500,  # æ”¹ä¸º 500
    nonmember_size=500,  # æ”¹ä¸º 500
    seed=42
)
```

### 2. ç´¢å¼•æ„å»ºè¯¦è§£

`build_index.py` ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š

```python
config_dict = {
    'corpus_path': '/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl',
    'save_dir': '/home/user/FlashRAG/indexes/scifact',
    'retrieval_method': 'bge',  # BGE ç¼–ç å™¨
    'retrieval_model_path': 'BAAI/bge-large-en-v1.5',
    'faiss_type': 'flat',  # ç²¾ç¡®æœç´¢
    'faiss_gpu': True,  # GPU åŠ é€Ÿ
}
```

**æ›´æ”¹ç¼–ç å™¨æ¨¡å‹ï¼š**
```python
# åœ¨ build_index.py ä¸­ä¿®æ”¹
'retrieval_model_path': 'BAAI/bge-base-en-v1.5',  # ä½¿ç”¨ base æ¨¡å‹
'embedding_dim': 768,  # base æ¨¡å‹çš„ç»´åº¦
```

### 3. å¤šè½®å¯¹è¯è¯¦è§£

#### åŸºæœ¬ä½¿ç”¨

```python
from mia_multi_turn_chat import create_mia_chat

# åˆ›å»ºå¯¹è¯å®ä¾‹
chat = create_mia_chat(
    model_path="/remote-home/RAG_Privacy/model/meta-llama/Llama-3.1-8B-Instruct",
    retrieval_method="bge",
    corpus_path="/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl",
    index_path="/home/user/FlashRAG/indexes/scifact",
    retrieval_topk=3
)

# å¸¦æ£€ç´¢çš„å¯¹è¯
result = chat.chat(
    user_query="What is the role of MDSC in myelodysplasia?",
    use_retrieval=True,
    topk=3,
    return_details=True
)

# è®¿é—®ç»“æœ
print(f"Response: {result['response']}")
print(f"Retrieved Doc IDs: {result['retrieved_doc_ids']}")
print(f"Answer Probabilities: {result['answer_probs']}")
print(f"Predicted Answer: {result['predicted_answer']}")
```

#### è¿”å›å€¼è¯¦è§£

```python
result = {
    'user_query': str,              # ç”¨æˆ·æŸ¥è¯¢
    'use_retrieval': bool,          # æ˜¯å¦ä½¿ç”¨æ£€ç´¢
    'response': str,                # ç”Ÿæˆçš„å›ç­”
    'retrieved_doc_ids': List[str], # æ£€ç´¢åˆ°çš„æ–‡æ¡£IDï¼ˆå¦‚æœä½¿ç”¨æ£€ç´¢ï¼‰
    'retrieved_documents': List[Dict],  # æ£€ç´¢åˆ°çš„å®Œæ•´æ–‡æ¡£
    'logits': torch.Tensor,         # ç¬¬ä¸€ä¸ªtokençš„logits [vocab_size]
    'answer_probs': Dict[str, float],  # A-Eçš„æ¦‚ç‡ {'A': 0.1, 'B': 0.2, ...}
    'predicted_answer': str         # é¢„æµ‹çš„ç­”æ¡ˆå­—æ¯
}
```

#### ä¸å¸¦æ£€ç´¢çš„å¯¹è¯

```python
# ä¸ä½¿ç”¨æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
result = chat.chat(
    user_query="Can you explain more?",
    use_retrieval=False
)
```

#### å¯¹è¯å†å²ç®¡ç†

```python
# æŸ¥çœ‹å¯¹è¯å†å²
history = chat.get_conversation_history()

# æ‰“å°å¯¹è¯å†å²
chat.print_conversation_history()

# é‡ç½®å¯¹è¯
chat.reset_conversation()
```

### 4. å•æ¬¡æŸ¥è¯¢çš„å®Œæ•´ Pipeline

```python
from flashrag.config import Config
from flashrag.utils import get_generator, get_retriever
from flashrag.prompt import PromptTemplate

# é…ç½®
config_dict = {
    'generator_model_path': '/remote-home/RAG_Privacy/model/meta-llama/Llama-3.1-8B-Instruct',
    'retrieval_method': 'bge',
    'corpus_path': '/home/user/FlashRAG/datasets/scifact/index_corpus.jsonl',
    'index_path': '/home/user/FlashRAG/indexes/scifact',
    'retrieval_topk': 3,
    'faiss_gpu': True,
}

config = Config(config_dict=config_dict)

# åˆå§‹åŒ–ç»„ä»¶
retriever = get_retriever(config)
generator = get_generator(config)
prompt_template = PromptTemplate(
    config,
    system_prompt=PromptTemplate.mia_system_prompt
)

# æŸ¥è¯¢
query = "Your question here"

# 1. æ£€ç´¢ï¼ˆè¿”å›æ–‡æ¡£IDï¼‰
results, scores, doc_ids = retriever._batch_search(
    query=[query],
    num=3,
    return_score=True,
    return_doc_ids=True  # æ–°å¢å‚æ•°
)

print(f"Retrieved document IDs: {doc_ids[0]}")

# 2. æ„å»º prompt
input_prompt = prompt_template.get_string(
    question=query,
    retrieval_result=results[0]
)

# 3. ç”Ÿæˆï¼ˆè¿”å›logitsï¼‰
output = generator.generate(
    [input_prompt],
    return_dict=True,  # è¿”å›è¯¦ç»†ä¿¡æ¯
    max_new_tokens=10
)

response = output['responses'][0]
logits = output['generated_token_logits'][0]  # [num_tokens, vocab_size]

# 4. æå–ç­”æ¡ˆæ¦‚ç‡
answer_tokens = ['A', 'B', 'C', 'D', 'E']
answer_token_ids = {
    token: generator.tokenizer.convert_tokens_to_ids(token)
    for token in answer_tokens
}

first_token_logits = logits[0]  # ç¬¬ä¸€ä¸ªtokençš„logits
answer_probs = {
    token: first_token_logits[token_id].item()
    for token, token_id in answer_token_ids.items()
}

print(f"Response: {response}")
print(f"Answer Probabilities: {answer_probs}")
```

---

## ğŸ“š API å‚è€ƒ

### MIAMultiTurnChat ç±»

#### åˆå§‹åŒ–

```python
from mia_multi_turn_chat import create_mia_chat

chat = create_mia_chat(
    model_path: str,           # LLM æ¨¡å‹è·¯å¾„
    retrieval_method: str,     # æ£€ç´¢æ–¹æ³• (é»˜è®¤ 'bge')
    corpus_path: str,          # è¯­æ–™åº“è·¯å¾„
    index_path: str,           # ç´¢å¼•è·¯å¾„
    retrieval_topk: int        # æ£€ç´¢æ–‡æ¡£æ•°é‡ (é»˜è®¤ 3)
)
```

#### æ–¹æ³•

##### `chat(user_query, use_retrieval=True, topk=3, return_details=True)`

æ‰§è¡Œä¸€è½®å¯¹è¯ã€‚

**å‚æ•°ï¼š**
- `user_query` (str): ç”¨æˆ·æŸ¥è¯¢
- `use_retrieval` (bool): æ˜¯å¦ä½¿ç”¨æ£€ç´¢ï¼ˆé»˜è®¤ Trueï¼‰
- `topk` (int): æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 3ï¼‰
- `return_details` (bool): æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆé»˜è®¤ Trueï¼‰

**è¿”å›ï¼š**
- dict: åŒ…å« response, retrieved_doc_ids, logits, answer_probs ç­‰

##### `reset_conversation()`

é‡ç½®å¯¹è¯å†å²ã€‚

##### `get_conversation_history()`

è·å–å¯¹è¯å†å²åˆ—è¡¨ã€‚

**è¿”å›ï¼š**
- List[Dict]: å¯¹è¯å†å²

##### `print_conversation_history()`

æ‰“å°æ ¼å¼åŒ–çš„å¯¹è¯å†å²ã€‚

---

### ä¿®æ”¹çš„ FlashRAG ç»„ä»¶

#### DenseRetriever

**æ–°å¢å‚æ•°ï¼š**

```python
retriever._batch_search(
    query: List[str],
    num: int = None,
    return_score: bool = False,
    return_doc_ids: bool = False  # æ–°å¢ï¼šè¿”å›æ–‡æ¡£ID
)
```

**è¿”å›å€¼ï¼š**
- å¦‚æœ `return_doc_ids=True` ä¸” `return_score=True`ï¼š
  - `(results, scores, doc_ids)` ä¸‰å…ƒç»„
- å¦‚æœåªæœ‰ `return_doc_ids=True`ï¼š
  - `(results, doc_ids)` äºŒå…ƒç»„

#### HFCausalLMGenerator

**å·²æœ‰åŠŸèƒ½ï¼ˆåˆ©ç”¨ç°æœ‰å‚æ•°ï¼‰ï¼š**

```python
generator.generate(
    input_list: List[str],
    return_dict: bool = True,  # è¿”å›è¯¦ç»†ä¿¡æ¯
    max_new_tokens: int = 10
)
```

**è¿”å›å€¼ï¼ˆå½“ return_dict=Trueï¼‰ï¼š**
```python
{
    'responses': List[str],                    # ç”Ÿæˆçš„æ–‡æœ¬
    'generated_token_ids': torch.Tensor,       # ç”Ÿæˆçš„token IDs
    'generated_token_logits': torch.Tensor,    # æ¯ä¸ªtokençš„logits
    'scores': List[List[float]]                # æ¦‚ç‡åˆ†æ•°
}
```

#### PromptTemplate

**æ–°å¢å±æ€§ï¼š**

```python
PromptTemplate.mia_system_prompt = (
    "You are a helpful assistant, below is a query from a user and some relevant documents."
    "Answer the question given the information in those documents."
    " Your answer should be short and concise. You can only output one letter from A, B, C, D, or E as your answer."
    "\nThe following are given documents.\n\n{reference}"
)
```

**ä½¿ç”¨æ–¹æ³•ï¼š**

```python
prompt_template = PromptTemplate(
    config,
    system_prompt=PromptTemplate.mia_system_prompt,
    user_prompt=PromptTemplate.base_user_prompt
)
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: corpus.jsonl çš„æ ¼å¼è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ

A: æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼Œå¿…é¡»åŒ…å« `_id`, `title`, `text` å­—æ®µï¼š

```json
{"_id": "4983", "title": "Document title", "text": "Document content", "metadata": {}}
```

### Q2: å¦‚ä½•æ›´æ”¹æˆå‘˜/éæˆå‘˜æ ·æœ¬çš„æ•°é‡ï¼Ÿ

A: ä¿®æ”¹ `prepare_mia_data.py` ä¸­çš„ `sample_member_nonmember` è°ƒç”¨ï¼š

```python
member_docs, nonmember_docs, index_corpus = sample_member_nonmember(
    corpus,
    member_size=500,   # æ”¹ä¸ºæ‰€éœ€æ•°é‡
    nonmember_size=500,
    seed=42
)
```

### Q3: å¦‚ä½•ä½¿ç”¨ä¸åŒçš„ LLM æ¨¡å‹ï¼Ÿ

A: ä¿®æ”¹é…ç½®ä¸­çš„ `generator_model_path`ï¼š

```python
config_dict = {
    'generator_model_path': '/path/to/your/model',
    'generator_model': 'your-model-name',
}
```

### Q4: å¦‚ä½•ä½¿ç”¨ä¸åŒçš„æ£€ç´¢å™¨ï¼Ÿ

A: ä¿®æ”¹é…ç½®ä¸­çš„ `retrieval_method` å’Œ `retrieval_model_path`ï¼š

```python
config_dict = {
    'retrieval_method': 'e5',  # æˆ–å…¶ä»–æ–¹æ³•
    'retrieval_model_path': 'path/to/e5/model',
}
```

### Q5: ç”Ÿæˆçš„ logits æ˜¯ä»€ä¹ˆï¼Ÿ

A: `generated_token_logits` æ˜¯æ¨¡å‹ä¸ºæ¯ä¸ªç”Ÿæˆçš„ token è®¡ç®—çš„æœªå½’ä¸€åŒ–åˆ†æ•°ï¼ˆlogitsï¼‰ï¼Œå½¢çŠ¶ä¸º `[num_generated_tokens, vocab_size]`ã€‚ç¬¬ä¸€ä¸ª token çš„ logits (`logits[0]`) å¯¹åº”äºç­”æ¡ˆé€‰é¡¹ï¼ˆA-Eï¼‰çš„åŸå§‹åˆ†æ•°ã€‚

### Q6: å¦‚ä½•æå–æ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Ÿ

A: å‚è€ƒä»¥ä¸‹ä»£ç ï¼š

```python
# è·å–ç­”æ¡ˆ token IDs
answer_token_ids = {
    token: generator.tokenizer.convert_tokens_to_ids(token)
    for token in ['A', 'B', 'C', 'D', 'E']
}

# è·å–ç¬¬ä¸€ä¸ªtokençš„logits
first_token_logits = output['generated_token_logits'][0][0]

# æå–æ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡ï¼ˆlogitså·²ç»è¿‡softmaxï¼‰
answer_probs = {
    token: first_token_logits[token_id].item()
    for token, token_id in answer_token_ids.items()
}
```

### Q7: å¤šè½®å¯¹è¯ä¸­çš„å†å²å¦‚ä½•ç®¡ç†ï¼Ÿ

A: `MIAMultiTurnChat` ç±»è‡ªåŠ¨ç»´æŠ¤å†å²ã€‚æ¯æ¬¡è°ƒç”¨ `chat()` éƒ½ä¼šï¼š
1. å°†ç”¨æˆ·æŸ¥è¯¢æ·»åŠ åˆ° `self.messages`
2. å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ° `self.messages`
3. å¦‚æœä½¿ç”¨æ£€ç´¢ï¼Œè¿˜ä¼šä¿å­˜æ£€ç´¢åˆ°çš„æ–‡æ¡£ID

å¯ä»¥é€šè¿‡ `get_conversation_history()` æŸ¥çœ‹æˆ– `reset_conversation()` é‡ç½®ã€‚

### Q8: å¦‚ä½•æ‰¹é‡å¤„ç†å¤šä¸ªæŸ¥è¯¢ï¼Ÿ

A: ä½¿ç”¨å¾ªç¯æˆ–æ‰¹å¤„ç†ï¼š

```python
queries = [...]  # æŸ¥è¯¢åˆ—è¡¨

results = []
for query in queries:
    result = chat.chat(query, use_retrieval=True)
    results.append(result)

    # æ¯ä¸ªæŸ¥è¯¢åé‡ç½®å¯¹è¯ï¼ˆå¦‚æœéœ€è¦ç‹¬ç«‹å¤„ç†ï¼‰
    # chat.reset_conversation()
```

### Q9: GPU å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: å¯ä»¥ï¼š
1. å‡å° `generator_batch_size`
2. å‡å° `retrieval_batch_size`
3. ä½¿ç”¨ `retrieval_use_fp16=True`
4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q10: å¦‚ä½•ä¿å­˜å®éªŒç»“æœï¼Ÿ

A: å°†ç»“æœä¿å­˜ä¸º JSONï¼š

```python
import json

results = []
for query in queries:
    result = chat.chat(query, use_retrieval=True)
    # è½¬æ¢ tensor ä¸º list
    result['logits'] = result['logits'].tolist() if result['logits'] is not None else None
    results.append(result)

with open('mia_results.json', 'w') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹è„šæœ¬

```python
#!/usr/bin/env python
"""
å®Œæ•´çš„ MIA å®éªŒç¤ºä¾‹
"""

from mia_multi_turn_chat import create_mia_chat
import json

# 1. åˆ›å»ºå¯¹è¯å®ä¾‹
chat = create_mia_chat()

# 2. åŠ è½½æŸ¥è¯¢æ•°æ®é›†
with open('/home/user/FlashRAG/datasets/scifact/queries.jsonl', 'r') as f:
    queries = [json.loads(line) for line in f]

# 3. å¯¹æˆå‘˜å’Œéæˆå‘˜æ ·æœ¬è¿›è¡Œæ¨ç†
results = []

for query_item in queries[:10]:  # å¤„ç†å‰10ä¸ª
    query_text = query_item['question']
    is_member = query_item['metadata']['is_member']
    doc_id = query_item['metadata']['doc_id']

    # æ‰§è¡Œæ¨ç†
    result = chat.chat(
        user_query=query_text,
        use_retrieval=True,
        topk=3
    )

    # ä¿å­˜ç»“æœ
    result_record = {
        'doc_id': doc_id,
        'is_member': is_member,
        'query': query_text,
        'response': result['response'],
        'retrieved_doc_ids': result['retrieved_doc_ids'],
        'answer_probs': result['answer_probs'],
        'predicted_answer': result['predicted_answer']
    }

    results.append(result_record)

    # é‡ç½®å¯¹è¯å†å²ï¼ˆæ¯ä¸ªæŸ¥è¯¢ç‹¬ç«‹ï¼‰
    chat.reset_conversation()

# 4. ä¿å­˜ç»“æœ
with open('mia_experiment_results.json', 'w') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… Processed {len(results)} queries")
print(f"ğŸ’¾ Results saved to mia_experiment_results.json")
```

---

## ğŸ”¬ æˆå‘˜æ¨ç†æ”»å‡»åˆ†æ

ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ ID å’Œç­”æ¡ˆæ¦‚ç‡è¿›è¡Œæˆå‘˜æ¨ç†æ”»å‡»åˆ†æï¼š

```python
import numpy as np

def analyze_membership_inference(results):
    """
    åˆ†ææˆå‘˜æ¨ç†æ”»å‡»æ•ˆæœ

    Args:
        results: åŒ…å« is_member, retrieved_doc_ids, answer_probs çš„ç»“æœåˆ—è¡¨
    """
    member_scores = []
    nonmember_scores = []

    for result in results:
        doc_id = result['doc_id']
        is_member = result['is_member']
        retrieved_ids = result['retrieved_doc_ids']

        # æˆå‘˜æ¨ç†ä¿¡å·ï¼šæ–‡æ¡£æ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
        is_retrieved = doc_id in retrieved_ids

        # è·å–æ£€ç´¢ä½ç½®ï¼ˆè¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
        if is_retrieved:
            position = retrieved_ids.index(doc_id)
            score = 1.0 / (position + 1)  # ä½ç½®è¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜
        else:
            score = 0.0

        if is_member:
            member_scores.append(score)
        else:
            nonmember_scores.append(score)

    # ç»Ÿè®¡
    print(f"æˆå‘˜æ ·æœ¬å¹³å‡åˆ†æ•°: {np.mean(member_scores):.4f}")
    print(f"éæˆå‘˜æ ·æœ¬å¹³å‡åˆ†æ•°: {np.mean(nonmember_scores):.4f}")
    print(f"åˆ†æ•°å·®å¼‚: {np.mean(member_scores) - np.mean(nonmember_scores):.4f}")

    # è®¡ç®— AUC
    from sklearn.metrics import roc_auc_score

    all_scores = member_scores + nonmember_scores
    all_labels = [1] * len(member_scores) + [0] * len(nonmember_scores)

    auc = roc_auc_score(all_labels, all_scores)
    print(f"AUC: {auc:.4f}")

    return auc

# ä½¿ç”¨
auc = analyze_membership_inference(results)
```

---

## ğŸ“ è”ç³»ä¸æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥æœ¬ README çš„å¸¸è§é—®é¢˜éƒ¨åˆ†
2. æŸ¥çœ‹ FlashRAG å®˜æ–¹æ–‡æ¡£
3. è¿è¡Œ `test_mia_pipeline.py` è¿›è¡Œè°ƒè¯•

---

## ğŸ“„ è®¸å¯è¯

éµå¾ª FlashRAG çš„åŸå§‹è®¸å¯è¯ã€‚

---

**Happy Experimenting! ğŸš€**
